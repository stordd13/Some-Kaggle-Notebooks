{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-03T05:34:47.307548Z","iopub.execute_input":"2022-06-03T05:34:47.308529Z","iopub.status.idle":"2022-06-03T05:34:47.344866Z","shell.execute_reply.started":"2022-06-03T05:34:47.30838Z","shell.execute_reply":"2022-06-03T05:34:47.343822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n\nimport sklearn\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV, GridSearchCV, StratifiedKFold, KFold\nfrom sklearn.feature_selection import SelectKBest, RFE, SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.metrics import confusion_matrix,plot_confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.linear_model import Perceptron, SGDClassifier, LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss, mean_absolute_error\nfrom sklearn.decomposition import PCA\nfrom sklearn import pipeline\nimport lightgbm as lgb\n\n## for visualization\nimport plotly.express as px\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport time \n\nplt.style.use('ggplot')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:34:47.420763Z","iopub.execute_input":"2022-06-03T05:34:47.421277Z","iopub.status.idle":"2022-06-03T05:34:50.804634Z","shell.execute_reply.started":"2022-06-03T05:34:47.421239Z","shell.execute_reply":"2022-06-03T05:34:50.80311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Notebook un peu plus clean (avec les déclaration des paramètres et variables au début) avec randomized search sur du xgb boost et/ou random forest sans pca ","metadata":{}},{"cell_type":"code","source":"# parameters \n#xgb\nparams = {\n        'min_child_weight': [1, 3, 5, 7, 10],\n        'gamma': [0, 0.1, 0.3, 0.5, 0.7, 1],\n        'subsample': [0, 0.5, 1.0],\n        'colsample_bytree': [0.2, 0.4, 0.6, 0.8, 1.0],\n        'max_depth': [2, 5, 7, 10, 15, 25, 50,100],\n        'eta': [0.05, 0.1, 0.3, 0.5,1],\n        'n_estimators': [100, 200, 500, 800, 1000, 1200, 1500, 2000]\n         }\n#rf\nspace = {\n       'n_estimators': [50, 100, 200, 400, 500, 800, 1000, 1200, 1500, 2000],\n       'max_features': ['auto', 'sqrt'],\n       'max_depth': [2,3,5,7, 10, 15, 25, 50, 70, 100, 150,200],\n       'min_samples_split': [1, 2, 5, 10,20],\n       'min_samples_leaf': [1, 2, 5, 10, 20],\n       'bootstrap': [True,False]\n        }\nseed = 42\nnp.random.seed(seed)\nkfolds = KFold(n_splits=5, shuffle=True, random_state=seed)\n\nmodels = {'xgb' : XGBClassifier(objective='binary:logistic'), 'rf': RandomForestClassifier()}\nmodel = 'xgb'","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:34:50.807209Z","iopub.execute_input":"2022-06-03T05:34:50.807794Z","iopub.status.idle":"2022-06-03T05:34:50.821071Z","shell.execute_reply.started":"2022-06-03T05:34:50.8077Z","shell.execute_reply":"2022-06-03T05:34:50.820123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/bioresponse/train.csv\")\ntest = pd.read_csv(\"../input/bioresponse/test.csv\")\nbenchmark= pd.read_csv(\"../input/bioresponse/svm_benchmark.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:34:51.618629Z","iopub.execute_input":"2022-06-03T05:34:51.61914Z","iopub.status.idle":"2022-06-03T05:34:53.693564Z","shell.execute_reply.started":"2022-06-03T05:34:51.619103Z","shell.execute_reply":"2022-06-03T05:34:53.692187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = train.Activity\nX = train.drop('Activity', axis=1)\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\ntest = scaler.transform(test)\n\nX_train,X_test, y_train,y_test = train_test_split(X,Y,test_size=0.20, random_state=seed)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:35:13.853698Z","iopub.execute_input":"2022-06-03T05:35:13.854182Z","iopub.status.idle":"2022-06-03T05:35:14.228609Z","shell.execute_reply.started":"2022-06-03T05:35:13.854143Z","shell.execute_reply":"2022-06-03T05:35:14.227444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n\npca = PCA(n_components=1000)\npca.fit(X_train)\nX_train, X_test = pca.transform(X_train), pca.transform(X_test)\ntest = pca.transform(test)\nend = time.time()\nprint(end-start)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:35:16.535532Z","iopub.execute_input":"2022-06-03T05:35:16.536697Z","iopub.status.idle":"2022-06-03T05:35:23.373831Z","shell.execute_reply.started":"2022-06-03T05:35:16.536647Z","shell.execute_reply":"2022-06-03T05:35:23.372696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n\nif model=='rf':\n    random_search = RandomizedSearchCV(estimator = models[model], param_distributions = space, scoring='neg_log_loss',\n                               n_iter = 100, cv = 10, verbose=2, random_state=seed, n_jobs = -1)\n    random_search.fit(X_train,y_train)\n    \nelif model=='xgb':\n    random_search = RandomizedSearchCV(estimator=models[model], param_distributions=params, verbose=2,  n_jobs=4,\n                                   scoring = 'neg_log_loss', cv=10, random_state=seed)\n    random_search.fit(X_train,y_train)\nelse:\n    print(\"no model has been defined\")\n    \n\nend = time.time()\nprint(end-start)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T05:35:32.770493Z","iopub.execute_input":"2022-06-03T05:35:32.770913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\n\nmodel = random_search.best_estimator_\nmodel.fit(X,Y)\nsub = model.predict_proba(test)\n\nend = time.time()\nprint(end-start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'MoleculeId': range(1,test.shape[0]+1),\n                        'PredictedProbability': sub[:,1]})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## on re train le \"meilleur model\" sur tout le train set et on fait notre prédiction sur le test set en entier \n#train = pd.read_csv(\"../input/bioresponse/train.csv\")\n#test = pd.read_csv(\"../input/bioresponse/test.csv\")\n#Y = train.Activity\n#X = train.drop('Activity', axis=1)\n#scaler = StandardScaler()\n##scaler.fit(X)\n#X = scaler.transform(X)\n#test = scaler.transform(test)\n\n#model.fit(X,Y)\n#clf.fit(X,Y)\n#sub = clf.predict_proba(test)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T15:23:12.974058Z","iopub.execute_input":"2022-05-31T15:23:12.974924Z","iopub.status.idle":"2022-05-31T15:25:23.167058Z","shell.execute_reply.started":"2022-05-31T15:23:12.97488Z","shell.execute_reply":"2022-05-31T15:25:23.166137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-05-31T15:25:23.17139Z","iopub.execute_input":"2022-05-31T15:25:23.172058Z","iopub.status.idle":"2022-05-31T15:25:23.187369Z","shell.execute_reply.started":"2022-05-31T15:25:23.171974Z","shell.execute_reply":"2022-05-31T15:25:23.186189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}