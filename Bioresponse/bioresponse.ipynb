{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nimport pandas as pd \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-10T12:17:04.470869Z","iopub.execute_input":"2022-05-10T12:17:04.472274Z","iopub.status.idle":"2022-05-10T12:17:04.480987Z","shell.execute_reply.started":"2022-05-10T12:17:04.472222Z","shell.execute_reply":"2022-05-10T12:17:04.480085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix,plot_confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.linear_model import Perceptron, SGDClassifier, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.svm import OneClassSVM\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score, log_loss\nfrom sklearn.decomposition import PCA\n\n## for visualization\nimport plotly.express as px\nimport matplotlib\nfrom datetime import datetime, date \nfrom datetime import timedelta\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom datetime import datetime, date \nfrom datetime import timedelta\nimport time\nimport shap\n\nimport random\nimport pickle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/bioresponse/train.csv\")\ndf_test = pd.read_csv(\"../input/bioresponse/test.csv\")\nbenchmark= pd.read_csv(\"../input/bioresponse/svm_benchmark.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.Activity.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T12:17:10.2466Z","iopub.execute_input":"2022-05-10T12:17:10.246887Z","iopub.status.idle":"2022-05-10T12:17:10.288633Z","shell.execute_reply.started":"2022-05-10T12:17:10.246856Z","shell.execute_reply":"2022-05-10T12:17:10.287748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.D4.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-10T12:17:10.29078Z","iopub.execute_input":"2022-05-10T12:17:10.291108Z","iopub.status.idle":"2022-05-10T12:17:10.307432Z","shell.execute_reply.started":"2022-05-10T12:17:10.291068Z","shell.execute_reply":"2022-05-10T12:17:10.306516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isna().sum().sort_values(ascending=False) ## not missing data ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sns.pairplot(df_train.iloc[:,:10])\n\n## linear dependance between : D6 & D9, D6 & D7 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_train.corrwith(df_train.Activity)\n#values = df_train.corrwith(df_train.Activity)[df_train.corrwith(df_train.Activity).values>0.05].index\n#values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#first part : fit a basic RF/xgb without tuning or features selection etc.. \n#to have a first log loss value to improve\n\n#seed = random.seed(10)\n\n#scaler = StandardScaler()\ntrain = df_train.drop('Activity', axis=1)\ntarget = df_train.Activity\n\n#train = scaler.fit_transform(train)\nX_train, X_test,y_train, y_test = train_test_split(train,target, test_size=0.3)\nrf = RandomForestClassifier()\nxgb = XGBClassifier()\n\n\nrf.fit(X_train,y_train)\npred_rf = rf.predict(X_test)\nscore_rf = accuracy_score(pred_rf,y_test)\nloss_rf = log_loss(pred_rf,y_test)\n\nxgb.fit(X_train,y_train)\npred_xgb = xgb.predict(X_test)\n\nscore_xgb = accuracy_score(pred_xgb, y_test)\nloss_xgb = log_loss(pred_xgb, y_test)\n\n#rf.feature_importances_\n\nprint(loss_rf, loss_xgb)\nprint(score_rf, score_xgb)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:50:03.104554Z","iopub.execute_input":"2022-04-27T13:50:03.10498Z","iopub.status.idle":"2022-04-27T13:50:14.870066Z","shell.execute_reply.started":"2022-04-27T13:50:03.104947Z","shell.execute_reply":"2022-04-27T13:50:14.868897Z"}}},{"cell_type":"code","source":"## let's try removing features with really low correlation with the target \n\n#df = df_train.copy()\n#df = df.loc[:,(df.corrwith(target)>0.05)]\n\n#scaler = StandardScaler()\n#train1 = df.drop('Activity', axis=1)\n#target1 = df.Activity\n\n#train1 = scaler.fit_transform(train1)\n#X_train1, X_test1,y_train1, y_test1 = train_test_split(train1,target1, test_size=0.3)\n#rf1 = RandomForestClassifier()\n#xgb1 = XGBClassifier()\n\n#rf1.fit(X_train1,y_train1)\n#pred_rf1 = rf1.predict(X_test1)\n#score_rf1 = accuracy_score(pred_rf1,y_test1)\n#loss_rf1 = log_loss(pred_rf1,y_test1)\n\n#xgb1.fit(X_train1,y_train1)\n#pred_xgb1 = xgb1.predict(X_test1)\n#loss_xgb1 = log_loss(pred_xgb1, y_test1)\n#score_xgb1 = accuracy_score(y_test,pred_xgb1)\n#rf.feature_importances_\n\n#print(loss_rf1, loss_xgb1)\n#print(score_xgb1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### trying some stuff : like features importance and shapley values for the PCA approach notebook \n\nrf = RandomForestClassifier()\ntrain = df_train.drop('Activity', axis=1)\ny_train = df_train.Activity \nX_train = df_train.drop('Activity', axis=1)\nX_test = df_test\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nrf.fit(X_train,y_train)\n\nrf.feature_importances_","metadata":{"execution":{"iopub.status.busy":"2022-05-10T12:17:11.908515Z","iopub.execute_input":"2022-05-10T12:17:11.908865Z","iopub.status.idle":"2022-05-10T12:17:14.374432Z","shell.execute_reply.started":"2022-05-10T12:17:11.908827Z","shell.execute_reply":"2022-05-10T12:17:14.373595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_feature_importance(importance,names,model_type):\n\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    plt.bar(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')\n    plt.show()\n\nplot_feature_importance(rf.feature_importances_,train.columns,'RANDOM FOREST')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T12:18:33.65196Z","iopub.execute_input":"2022-05-10T12:18:33.652238Z","iopub.status.idle":"2022-05-10T12:18:33.716675Z","shell.execute_reply.started":"2022-05-10T12:18:33.65221Z","shell.execute_reply":"2022-05-10T12:18:33.715586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install shap ","metadata":{"execution":{"iopub.status.busy":"2022-05-10T10:56:13.983036Z","iopub.execute_input":"2022-05-10T10:56:13.983336Z","iopub.status.idle":"2022-05-10T10:56:26.202158Z","shell.execute_reply.started":"2022-05-10T10:56:13.983303Z","shell.execute_reply":"2022-05-10T10:56:26.201284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### SHAPLEY VALUES \n\nshap_values = shap.TreeExplainer(rf).shap_values(X_train)\nshap.summary_plot(shap_values, X_train, plot_type='bar')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T11:04:26.162248Z","iopub.execute_input":"2022-05-10T11:04:26.162688Z","iopub.status.idle":"2022-05-10T11:08:54.628586Z","shell.execute_reply.started":"2022-05-10T11:04:26.162644Z","shell.execute_reply":"2022-05-10T11:08:54.627606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = rf.predict_proba(df_test)\n#submission = pd.DataFrame({\"id\": df_test.index, \"target\": sample})\n#submission.drop('id', axis=1, inplace=True) \n#submission.to_csv(\"submission.csv\", index=False)\n\nsubmission = pd.DataFrame({'MoleculeId': np.arange(1, len(sample)+1),\n                        'PredictedProbability': sample[:, 0]})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:54:10.422759Z","iopub.execute_input":"2022-04-27T13:54:10.423085Z","iopub.status.idle":"2022-04-27T13:54:10.554026Z","shell.execute_reply.started":"2022-04-27T13:54:10.423048Z","shell.execute_reply":"2022-04-27T13:54:10.552808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-04-27T13:54:53.51371Z","iopub.execute_input":"2022-04-27T13:54:53.514051Z","iopub.status.idle":"2022-04-27T13:54:53.534838Z","shell.execute_reply.started":"2022-04-27T13:54:53.51401Z","shell.execute_reply":"2022-04-27T13:54:53.534056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}